{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "degradation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ba9af6372e8499088084be2d1d0d5bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cf80fe1821f4d4c9cd04487b2a6c648",
              "IPY_MODEL_9a2fb3018d3e4342be51d3c5be08c637",
              "IPY_MODEL_4dccd7ac4fa5426581abacc849037ef5"
            ],
            "layout": "IPY_MODEL_ccf2867bf7da4a0da92caea1f238b6e1"
          }
        },
        "1cf80fe1821f4d4c9cd04487b2a6c648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d19cb4218064d5aae15197db8fd3103",
            "placeholder": "​",
            "style": "IPY_MODEL_90a508de50cf4c908933db2a572e7727",
            "value": ""
          }
        },
        "9a2fb3018d3e4342be51d3c5be08c637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ff7ce1a1088457ca48aea11a9fbd1e0",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a965844f4174c3c828bf5c42843b0ab",
            "value": 10000
          }
        },
        "4dccd7ac4fa5426581abacc849037ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ffad4073c6e4851a20cb343a5497256",
            "placeholder": "​",
            "style": "IPY_MODEL_5d72d316e9f940bea235f39a2934bed5",
            "value": " 10048/? [1:24:54&lt;00:00,  2.00it/s]"
          }
        },
        "ccf2867bf7da4a0da92caea1f238b6e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d19cb4218064d5aae15197db8fd3103": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90a508de50cf4c908933db2a572e7727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ff7ce1a1088457ca48aea11a9fbd1e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a965844f4174c3c828bf5c42843b0ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ffad4073c6e4851a20cb343a5497256": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d72d316e9f940bea235f39a2934bed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "load model"
      ],
      "metadata": {
        "id": "Km5ZuaX1oF4P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J4LMLCHO88h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "import scipy.ndimage\n",
        "from scipy import misc\n",
        "from glob import glob\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import skimage\n",
        "import imageio\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# importing metadata\n",
        "data_dir = \"\"\n",
        "metadata = pd.read_csv(data_dir + 'HAM10000_metadata.csv')\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(metadata['dx'])\n",
        "LabelEncoder()\n",
        "print(\"Classes:\", list(le.classes_))\n",
        "\n",
        "metadata['label'] = le.transform(metadata[\"dx\"])\n",
        "metadata.sample(10)\n",
        "\n",
        "dest_dir = \"\"\n",
        "\n",
        "label = [ 'akiec', 'bcc','bkl','df','mel', 'nv',  'vasc']\n",
        "classes = [ 'actinic keratoses', 'basal cell carcinoma', 'benign keratosis-like lesions',\n",
        "           'dermatofibroma','melanoma', 'melanocytic nevi', 'vascular lesions']\n",
        "\n",
        "def estimate_weights_mfb(label):\n",
        "  class_weights = np.zeros_like(label, dtype=np.float)\n",
        "  counts = np.zeros_like(label)\n",
        "  for i, l in enumerate(label):\n",
        "    counts[i] = metadata[metadata['dx'] == str(l)]['dx'].value_counts()[0]\n",
        "  counts = counts.astype(np.float)\n",
        "  median_freq = np.median(counts)\n",
        "  for i, label in enumerate(label):\n",
        "    class_weights[i] = median_freq/counts[i]\n",
        "  return class_weights\n",
        "\n",
        "classweight = estimate_weights_mfb(label)\n",
        "for i in range(len(label)):\n",
        "    print(label[i],\":\", classweight[i])\n",
        "\n",
        "norm_mean = (0.4914, 0.4822, 0.4465)\n",
        "norm_std = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "batch_size = 10\n",
        "validation_batch_size = 10\n",
        "\n",
        "class_weights = estimate_weights_mfb(label)\n",
        "class_weights = torch.FloatTensor(class_weights)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "                    transforms.Resize((224,224)),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    transforms.RandomRotation(degrees=60),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(norm_mean, norm_std),\n",
        "                    ])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "                    transforms.Resize((224,224)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                    ])\n",
        "\n",
        "train_indices = np.loadtxt(\"/train_indices.txt\").astype(np.int)\n",
        "val_indices = np.loadtxt(\"/val_indices.txt\").astype(np.int)\n",
        "test_indices = np.loadtxt(\"/test_indices.txt\").astype(np.int)\n",
        "\n",
        "SubsetRandomSampler = torch.utils.data.sampler.SubsetRandomSampler\n",
        "\n",
        "train_samples = SubsetRandomSampler(train_indices)\n",
        "val_samples = SubsetRandomSampler(val_indices)\n",
        "test_samples = SubsetRandomSampler(test_indices)\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder(root=dest_dir, transform = transform_train)\n",
        "train_data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False,num_workers=1, sampler= train_samples)\n",
        "validation_data_loader = torch.utils.data.DataLoader(dataset, batch_size=validation_batch_size, shuffle=False, sampler=val_samples)\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder(root= dest_dir, transform=transform_test)\n",
        "test_data_loader = torch.utils.data.DataLoader(dataset, batch_size=validation_batch_size, shuffle=False, sampler=test_samples)\n",
        "\n",
        "\n",
        "## define CNN\n",
        "num_classes = len(classes)\n",
        "\n",
        "vgg = torchvision.models.vgg16(pretrained = True)\n",
        "\n",
        "vgg.classifier[-1] = nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
        "vgg = vgg.to(device)\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight = class_weights.to(device))\n",
        "optimizer = optim.Adam(vgg.parameters(), lr = 1e-6)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def get_accuracy(predicted, labels):\n",
        "    batch_len, correct= 0, 0\n",
        "    batch_len = labels.size(0)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    return batch_len, correct\n",
        "\n",
        "def evaluate(model, val_loader):\n",
        "    losses= 0\n",
        "    num_samples_total=0\n",
        "    correct_total=0\n",
        "    model.eval()\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        out = model(inputs)\n",
        "        _, predicted = torch.max(out, 1)\n",
        "        loss = criterion(out, labels)\n",
        "        losses += loss.item()\n",
        "        b_len, corr = get_accuracy(predicted, labels)\n",
        "        num_samples_total +=b_len\n",
        "        correct_total +=corr\n",
        "    accuracy = correct_total/num_samples_total\n",
        "    losses = losses/len(val_loader)\n",
        "    return losses, accuracy\n",
        "\n",
        "vgg.load_state_dict(torch.load('/.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from torchvision.transforms import ToTensor\n",
        "from tqdm import tqdm\n",
        "\n",
        "from attribution_bottleneck.utils.misc import *\n",
        "from attribution_bottleneck.utils.baselines import Mean\n",
        "from attribution_bottleneck.evaluate.perturber import *\n",
        "from sklearn.metrics import auc\n",
        "warn_baseline_prob = 0.05\n",
        "tile_size = (56,56)"
      ],
      "metadata": {
        "id": "ZjrjyPe1oNOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_np(t: torch.Tensor):\n",
        "    t = t.detach()\n",
        "    if t.is_cuda:\n",
        "        t = t.cpu()\n",
        "    return t.numpy()\n",
        "\n",
        "def eval_np(img_t):\n",
        "        \"\"\" pass the tensor through the network and return the scores as a numpyarray w/o batch dimension (1D shape)\"\"\"\n",
        "        return to_np(vgg(img_t))[0]\n",
        "\n",
        "def show_img(img, title=\"\", place=None):\n",
        "    img = to_np_img(img)\n",
        "    if place is None:\n",
        "        place = plt\n",
        "    try:\n",
        "        if len(img.shape) == 3 and img.shape[2] == 1:\n",
        "            # remove single grey channel\n",
        "            img = img[...,0]\n",
        "\n",
        "        if len(img.shape) == 2:\n",
        "            place.imshow(img, cmap=\"Greys_r\")\n",
        "        else:\n",
        "            place.imshow(img)\n",
        "    except TypeError:\n",
        "        print(\"type error: shape is {}\".format(img.shape))\n",
        "        raise TypeError\n",
        "\n",
        "    if not isinstance(place, Axes):\n",
        "        place.title(title)\n",
        "        plt.show()\n",
        "    else:\n",
        "        place.set_title(title)\n"
      ],
      "metadata": {
        "id": "CG_qbzUQU7pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gradcam"
      ],
      "metadata": {
        "id": "ljBc1U_FPqGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_layers = [vgg.features[29]]  ## for vgg16\n",
        "\n",
        "save_model_results = []\n",
        "for i, data in enumerate(train_data_loader):\n",
        "  gc_inputs, gc_labels = data\n",
        "  gc_target = []\n",
        "  for lbl in gc_labels:\n",
        "    gc_target.append([ClassifierOutputTarget(lbl)])\n",
        "\n",
        "  print(\"###################\", i)\n",
        "  if (i==100): break\n",
        "\n",
        "  for j, im in enumerate(gc_inputs):    \n",
        "    with GradCAM(model=vgg,\n",
        "                      target_layers=target_layers,\n",
        "                      use_cuda=torch.cuda.is_available()) as cam:\n",
        "\n",
        "      cam.batch_size = 32\n",
        "      input_tensor = im.unsqueeze(0)\n",
        "\n",
        "      grayscale_cam = cam(input_tensor=input_tensor,\n",
        "                      targets=gc_target[j], \n",
        "                      aug_smooth=True,\n",
        "                      eigen_smooth=True)\n",
        "      \n",
        "      # Here grayscale_cam has only one image in the batch]\n",
        "      grayscale_cam = grayscale_cam[0, :]\n",
        "      im = im.swapaxes(0,1)\n",
        "      im = im.swapaxes(1,2)\n",
        "      im = im.cpu().numpy()\n",
        "\n",
        "      ## degradation ##\n",
        "      img_t = im\n",
        "      hmap = grayscale_cam\n",
        "      \n",
        "      tts = ToTensor()\n",
        "      img_t = tts(img_t).unsqueeze(0)\n",
        "      img_t = img_t.cuda()\n",
        "      img = to_np_img(img_t)\n",
        "\n",
        "      baseline_img = Mean().apply(img)\n",
        "      baseline_t = to_img_tensor(baseline_img, device=img_t.device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        initial_out = eval_np(img_t)\n",
        "      top1 = np.argmax(initial_out)\n",
        "      initial_val = initial_out[top1]\n",
        "      baseline_val = eval_np(baseline_t)[top1]\n",
        "\n",
        "      perturber = PixelPerturber(img_t, baseline_img) if (tile_size is None or tile_size == (1, 1)) else GridPerturber(img_t, baseline_t, tile_size)\n",
        "      idxes = perturber.get_idxes(hmap)\n",
        "\n",
        "      max_steps = len(idxes)\n",
        "      n_steps = 200\n",
        "      progbar = False\n",
        "\n",
        "      do_steps = int(max_steps * 1.0)\n",
        "      parts = np.linspace(0, 1, n_steps)\n",
        "      parts_int = [int(p) for p in np.round(parts*max_steps)]\n",
        "      min_value = initial_val\n",
        "      min_degraded_t = img_t\n",
        "\n",
        "      parts = [0]\n",
        "      perturbed_ts = [img_t]\n",
        "      for step in tqdm(range(do_steps), desc=\"Perturbing\", disable=not progbar):\n",
        "        perturber.perturbe(*idxes[step])\n",
        "        if step in parts_int:\n",
        "            perturbed_ts.append(perturber.get_current().clone())\n",
        "\n",
        "      perturbed_ts = torch.cat(perturbed_ts, 0)\n",
        "      with torch.no_grad():\n",
        "        model_results = to_np(vgg(perturbed_ts))\n",
        "\n",
        "      model_results = model_results[:, top1]\n",
        "      model_results = np.array(model_results)\n",
        "      model_results = model_results/max(model_results)\n",
        "      save_model_results.append(model_results)\n",
        "\n",
        "  \n",
        "np_result = np.array(save_model_results)\n",
        "result = np_result.mean(axis=0)\n",
        "result_normalize = (result-min(result))/(max(result) - min(result)) \n",
        "\n",
        "x = np.array(range(len(result)))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"gradcam\" + \",AUC=\" + str(round(auc(x, result_normalize)/(result_normalize[0]*len(result_normalize)),3)))\n",
        "plt.ylabel('normalized model score')\n",
        "plt.xlabel('level of degradation[%]')\n",
        "plt.ylim([-0.05,1.05])\n",
        "plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "plt.xticks([0, 20, 40, 60, 80, 100])\n",
        "new_x = x/max(x)*100\n",
        "plt.plot(new_x, result_normalize, color=\"gray\")\n",
        "plt.savefig(\"\")\n",
        "gradcam_deletion_result = result_normalize\n",
        "np.savetxt('/gradcam_deletion_result.txt', gradcam_deletion_result)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Pwi6sQWoY_zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RISE"
      ],
      "metadata": {
        "id": "6MzSfhWfyWtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from explanations import RISE\n",
        "from RISE_utils import *\n",
        "from tqdm import tqdm\n",
        "\n",
        "args = Dummy()\n",
        "\n",
        "args.input_size = (224, 224)\n",
        "args.gpu_batch = 1\n",
        "\n",
        "explainer = RISE(vgg, args.input_size, args.gpu_batch)\n",
        "\n",
        "def explain(idx, img, target):\n",
        "    img = img.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "      saliency = explainer(img.cuda()).cpu().numpy()\n",
        "      sal = saliency[target]\n",
        "\n",
        "    return sal"
      ],
      "metadata": {
        "id": "YnDfmgVhyaQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maskspath = 'masks.npy'\n",
        "generate_new = True\n",
        "\n",
        "if generate_new or not os.path.isfile(maskspath):\n",
        "    explainer.generate_masks(N=3000, s=8, p1=0.1, savepath=maskspath)\n",
        "else:\n",
        "    explainer.load_masks(maskspath)\n",
        "    print('Masks are loaded.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqsV_T0Czn4z",
        "outputId": "4d30801b-fcf3-4b8d-a818-fea0cd830ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating filters: 100%|██████████| 3000/3000 [00:22<00:00, 131.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tile_size = (56, 56)\n",
        "save_model_results = []\n",
        "for i, data in enumerate(train_data_loader):\n",
        "  inputs, labels = data\n",
        "\n",
        "  print(\"###################\", i)\n",
        "  if (i==100): break\n",
        "\n",
        "  for j, im in enumerate(inputs):\n",
        "    hmap = explain(i, im, labels[j])\n",
        "    \n",
        "    im = im.swapaxes(0,1)\n",
        "    im = im.swapaxes(1,2)\n",
        "    im = im.cpu().numpy()\n",
        "\n",
        "    ## degradation ##\n",
        "    img_t = im\n",
        "    \n",
        "    tts = ToTensor()\n",
        "    img_t = tts(img_t).unsqueeze(0)\n",
        "    img_t = img_t.cuda()\n",
        "    img = to_np_img(img_t)\n",
        "\n",
        "    baseline_img = Mean().apply(img)\n",
        "    baseline_t = to_img_tensor(baseline_img, device=img_t.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      initial_out = eval_np(img_t)\n",
        "    top1 = np.argmax(initial_out)\n",
        "    initial_val = initial_out[top1]\n",
        "    baseline_val = eval_np(baseline_t)[top1]\n",
        "\n",
        "    perturber = PixelPerturber(img_t, baseline_img) if (tile_size is None or tile_size == (1, 1)) else GridPerturber(img_t, baseline_t, tile_size)\n",
        "    idxes = perturber.get_idxes(hmap)\n",
        "\n",
        "    max_steps = len(idxes)\n",
        "    n_steps = 200\n",
        "    progbar = False\n",
        "\n",
        "    do_steps = int(max_steps * 1.0)\n",
        "    parts = np.linspace(0, 1, n_steps)\n",
        "    parts_int = [int(p) for p in np.round(parts*max_steps)]\n",
        "    min_value = initial_val\n",
        "    min_degraded_t = img_t\n",
        "\n",
        "    parts = [0]\n",
        "    perturbed_ts = [img_t]\n",
        "    for step in tqdm(range(do_steps), desc=\"Perturbing\", disable=not progbar):\n",
        "      perturber.perturbe(*idxes[step])\n",
        "      if step in parts_int:\n",
        "          perturbed_ts.append(perturber.get_current().clone())\n",
        "\n",
        "    perturbed_ts = torch.cat(perturbed_ts, 0)\n",
        "    with torch.no_grad():\n",
        "      model_results = to_np(vgg(perturbed_ts))\n",
        "\n",
        "    model_results = model_results[:, top1]\n",
        "    model_results = np.array(model_results)\n",
        "    model_results = model_results/max(model_results)\n",
        "    save_model_results.append(model_results)\n",
        "\n",
        "  \n",
        "np_result = np.array(save_model_results)\n",
        "result = np_result.mean(axis=0)\n",
        "result_normalize = (result-min(result))/(max(result) - min(result)) \n",
        "\n",
        "x = np.array(range(len(result)))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"RISE\" + \",AUC=\" + str(round(auc(x, result_normalize)/(result_normalize[0]*len(result_normalize)),3)))\n",
        "plt.ylabel('normalized model score')\n",
        "plt.xlabel('level of degradation[%]')\n",
        "plt.ylim([-0.05,1.05])\n",
        "plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "plt.xticks([0, 20, 40, 60, 80, 100])\n",
        "new_x = x/max(x)*100\n",
        "plt.plot(new_x, result_normalize, color=\"gray\")  \n",
        "plt.savefig(\"\")\n",
        "rise_deletion_result = result_normalize\n",
        "np.savetxt('/rise_deletion_result.txt', rise_deletion_result)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dJyzrVuC0iUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extremal perturbation"
      ],
      "metadata": {
        "id": "zMY8nnuBCvuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchray.attribution.extremal_perturbation import extremal_perturbation, contrastive_reward\n",
        "from torchray.benchmark import get_example_data, plot_example\n",
        "from torchray.utils import get_device, imsc"
      ],
      "metadata": {
        "id": "UZooDFzmCw-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tile_size = (56, 56)\n",
        "save_model_results = []\n",
        "for i, data in enumerate(train_data_loader):\n",
        "  inputs, labels = data\n",
        "\n",
        "  print(\"###################\", i)\n",
        "  if (i==100): break\n",
        "\n",
        "  for j, im in enumerate(inputs):\n",
        "    im = im.unsqueeze(0)\n",
        "    im = im.to(device)\n",
        "    target = labels[j].tolist()\n",
        "\n",
        "    masks_1, _ = extremal_perturbation(\n",
        "      vgg, im, target,\n",
        "      reward_func=contrastive_reward,\n",
        "      debug=False,\n",
        "      areas=[0.07],\n",
        "    )\n",
        "\n",
        "    mask_img = masks_1.squeeze()\n",
        "    hmap = mask_img.cpu().numpy()\n",
        "\n",
        "    ## degradation ##\n",
        "    img_t = im\n",
        "    \n",
        "    img_t = img_t.cuda()\n",
        "    img = to_np_img(img_t)\n",
        "\n",
        "    baseline_img = Mean().apply(img)\n",
        "    baseline_t = to_img_tensor(baseline_img, device=img_t.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      initial_out = eval_np(img_t)\n",
        "    top1 = np.argmax(initial_out)\n",
        "    initial_val = initial_out[top1]\n",
        "    baseline_val = eval_np(baseline_t)[top1]\n",
        "\n",
        "    perturber = PixelPerturber(img_t, baseline_img) if (tile_size is None or tile_size == (1, 1)) else GridPerturber(img_t, baseline_t, tile_size)\n",
        "    idxes = perturber.get_idxes(hmap)\n",
        "\n",
        "    max_steps = len(idxes)\n",
        "    n_steps = 200\n",
        "    progbar = False\n",
        "\n",
        "    do_steps = int(max_steps * 1.0)\n",
        "    parts = np.linspace(0, 1, n_steps)\n",
        "    parts_int = [int(p) for p in np.round(parts*max_steps)]\n",
        "    min_value = initial_val\n",
        "    min_degraded_t = img_t\n",
        "\n",
        "    parts = [0]\n",
        "    perturbed_ts = [img_t]\n",
        "    for step in tqdm(range(do_steps), desc=\"Perturbing\", disable=not progbar):\n",
        "      perturber.perturbe(*idxes[step])\n",
        "      if step in parts_int:\n",
        "          perturbed_ts.append(perturber.get_current().clone())\n",
        "\n",
        "    perturbed_ts = torch.cat(perturbed_ts, 0)\n",
        "    with torch.no_grad():\n",
        "      model_results = to_np(vgg(perturbed_ts))\n",
        "\n",
        "    model_results = model_results[:, top1]\n",
        "    model_results = np.array(model_results)\n",
        "    model_results = model_results/max(model_results)\n",
        "    save_model_results.append(model_results)\n",
        "\n",
        "  \n",
        "np_result = np.array(save_model_results)\n",
        "result = np_result.mean(axis=0)\n",
        "result_normalize = (result-min(result))/(max(result) - min(result)) \n",
        "\n",
        "x = np.array(range(len(result)))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Extremal Perturbation\" + \",AUC=\" + str(round(auc(x, result_normalize)/(result_normalize[0]*len(result_normalize)),3)))\n",
        "plt.ylabel('normalized model score')\n",
        "plt.xlabel('level of degradation[%]')\n",
        "plt.ylim([-0.05,1.05])\n",
        "plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "plt.xticks([0, 20, 40, 60, 80, 100])\n",
        "new_x = x/max(x)*100\n",
        "plt.plot(new_x, result_normalize, color=\"gray\")  \n",
        "plt.savefig(\"\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7IZ3mHQZDIDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ext_deletion_result = result_normalize\n",
        "np.savetxt('/content/drive/MyDrive/HAM/ext_deletion_result.txt', ext_deletion_result)"
      ],
      "metadata": {
        "id": "HXSnlMYYlRme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## scoreCAM"
      ],
      "metadata": {
        "id": "OUNUi3K7JwhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import *\n",
        "from cam.scorecam import *"
      ],
      "metadata": {
        "id": "Y_vs8LMuJxWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tile_size = (56,56)\n",
        "\n",
        "save_model_results = []\n",
        "for i, data in enumerate(train_data_loader):\n",
        "  inputs, labels = data\n",
        "  print(\"###################\", i)\n",
        "  if (i==100): break\n",
        "\n",
        "  for j, im in enumerate(inputs):\n",
        "    vgg_model_dict = dict(type='vgg16', arch=vgg, layer_name='features_29',input_size=(224, 224))\n",
        "    vgg_scorecam = ScoreCAM(vgg_model_dict)\n",
        "    input_tensor = im.unsqueeze(0)\n",
        "    input_tensor = input_tensor.cuda()\n",
        "\n",
        "    scorecam_map = vgg_scorecam(input_tensor)\n",
        "    scorecam_map = scorecam_map.cpu().squeeze()\n",
        "\n",
        "    im = im.swapaxes(0,1)\n",
        "    im = im.swapaxes(1,2)\n",
        "    im = im.cpu().numpy()\n",
        "\n",
        "    ## degradation ##\n",
        "    img_t = im\n",
        "    hmap = scorecam_map\n",
        "    \n",
        "    tts = ToTensor()\n",
        "    img_t = tts(img_t).unsqueeze(0)\n",
        "    img_t = img_t.cuda()\n",
        "    img = to_np_img(img_t)\n",
        "\n",
        "    baseline_img = Mean().apply(img)\n",
        "    baseline_t = to_img_tensor(baseline_img, device=img_t.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      initial_out = eval_np(img_t)\n",
        "    top1 = np.argmax(initial_out)\n",
        "    initial_val = initial_out[top1]\n",
        "    baseline_val = eval_np(baseline_t)[top1]\n",
        "\n",
        "    perturber = PixelPerturber(img_t, baseline_img) if (tile_size is None or tile_size == (1, 1)) else GridPerturber(img_t, baseline_t, tile_size)\n",
        "    idxes = perturber.get_idxes(hmap)\n",
        "\n",
        "    max_steps = len(idxes)\n",
        "    n_steps = 200\n",
        "    progbar = False\n",
        "\n",
        "    do_steps = int(max_steps * 1.0)\n",
        "    parts = np.linspace(0, 1, n_steps)\n",
        "    parts_int = [int(p) for p in np.round(parts*max_steps)]\n",
        "    min_value = initial_val\n",
        "    min_degraded_t = img_t\n",
        "\n",
        "    parts = [0]\n",
        "    perturbed_ts = [img_t]\n",
        "    for step in tqdm(range(do_steps), desc=\"Perturbing\", disable=not progbar):\n",
        "      perturber.perturbe(*idxes[step])\n",
        "      if step in parts_int:\n",
        "          perturbed_ts.append(perturber.get_current().clone())\n",
        "\n",
        "    perturbed_ts = torch.cat(perturbed_ts, 0)\n",
        "    with torch.no_grad():\n",
        "      model_results = to_np(vgg(perturbed_ts))\n",
        "\n",
        "    model_results = model_results[:, top1]\n",
        "    model_results = np.array(model_results)\n",
        "    model_results = model_results/max(model_results)\n",
        "    save_model_results.append(model_results)\n",
        "\n",
        "  \n",
        "np_result = np.array(save_model_results)\n",
        "result = np_result.mean(axis=0)\n",
        "result_normalize = (result-min(result))/(max(result) - min(result)) \n",
        "\n",
        "x = np.array(range(len(result)))\n",
        "plt.title(\"ScoreCAM\" + \",AUC=\" + str(round(auc(x, result_normalize)/(result_normalize[0]*len(result_normalize)),3)))\n",
        "plt.ylabel('normalized model score')\n",
        "plt.xlabel('level of degradation[%]')\n",
        "plt.ylim([-0.05,1.05])\n",
        "plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "plt.xticks([0, 20, 40, 60, 80, 100])\n",
        "new_x = x/max(x)*100\n",
        "plt.plot(new_x, result_normalize)  \n",
        "plt.savefig(\"\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vYFB5vKuJ6Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorecam_deletion_result = result_normalize\n",
        "np.savetxt('//scorecam_deletion_result.txt', scorecam_deletion_result)"
      ],
      "metadata": {
        "id": "Rl7gT7-iq0A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IBA"
      ],
      "metadata": {
        "id": "iVvG9Gxg4p_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IBA\n",
        "from IBA.pytorch import IBA, tensor_to_np_img\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Compose, CenterCrop, ToTensor, Resize, Normalize\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "q7dYYwWX4x1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagenet_dir = \"\"\n",
        "\n",
        "dev = torch.device('cuda:0')\n",
        "\n",
        "vgg.to(dev).eval()\n",
        "\n",
        "\n",
        "image_size = 224\n",
        "    \n",
        "trainset = ImageFolder(\n",
        "    os.path.join(imagenet_dir),\n",
        "    transform=Compose([\n",
        "        CenterCrop(256), Resize(image_size), ToTensor(), \n",
        "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]))\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=1)"
      ],
      "metadata": {
        "id": "ZvHrYni56fJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iba = IBA(vgg.features[17])\n",
        "\n",
        "iba.reset_estimate()\n",
        "\n",
        "iba.estimate(vgg, trainloader, device=dev, n_samples=10000, progbar=True)\n",
        "\n",
        "neuron = (12, 3, 4)\n",
        "print(\"Neuron at position {:} has mean {:.2f} and std {:.2f}\".format(\n",
        "    neuron, iba.estimator.mean()[neuron],  iba.estimator.std()[neuron]))\n",
        "\n",
        "iba.estimator.n_samples()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "0ba9af6372e8499088084be2d1d0d5bc",
            "1cf80fe1821f4d4c9cd04487b2a6c648",
            "9a2fb3018d3e4342be51d3c5be08c637",
            "4dccd7ac4fa5426581abacc849037ef5",
            "ccf2867bf7da4a0da92caea1f238b6e1",
            "0d19cb4218064d5aae15197db8fd3103",
            "90a508de50cf4c908933db2a572e7727",
            "0ff7ce1a1088457ca48aea11a9fbd1e0",
            "5a965844f4174c3c828bf5c42843b0ab",
            "5ffad4073c6e4851a20cb343a5497256",
            "5d72d316e9f940bea235f39a2934bed5"
          ]
        },
        "id": "Pp4zmdmK6qa0",
        "outputId": "fac57d6b-ae77-45bd-be55-1692f36b27e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ba9af6372e8499088084be2d1d0d5bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neuron at position (12, 3, 4) has mean -6.96 and std 12.59\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10048"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tile_size = (56,56)\n",
        "\n",
        "save_model_results = []\n",
        "for i, data in enumerate(train_data_loader):\n",
        "  inputs, labels = data\n",
        "  print(\"###################\", i)\n",
        "  if (i==100): break\n",
        "\n",
        "  for j, im in enumerate(inputs):\n",
        "    target = labels[j]\n",
        "\n",
        "    model_loss_closure = lambda x: -torch.log_softmax(vgg(x), 1)[:, target].mean()\n",
        "    hmap = iba.analyze(im[None].to(dev), model_loss_closure) \n",
        "\n",
        "    im = im.swapaxes(0,1)\n",
        "    im = im.swapaxes(1,2)\n",
        "    im = im.cpu().numpy()\n",
        "\n",
        "    ## degradation ##\n",
        "    img_t = im\n",
        "    \n",
        "    tts = ToTensor()\n",
        "    img_t = tts(img_t).unsqueeze(0)\n",
        "    img_t = img_t.cuda()\n",
        "    img = to_np_img(img_t)\n",
        "\n",
        "    baseline_img = Mean().apply(img)\n",
        "    baseline_t = to_img_tensor(baseline_img, device=img_t.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      initial_out = eval_np(img_t)\n",
        "    top1 = np.argmax(initial_out)\n",
        "    initial_val = initial_out[top1]\n",
        "    baseline_val = eval_np(baseline_t)[top1]\n",
        "\n",
        "    perturber = PixelPerturber(img_t, baseline_img) if (tile_size is None or tile_size == (1, 1)) else GridPerturber(img_t, baseline_t, tile_size)\n",
        "    idxes = perturber.get_idxes(hmap)\n",
        "\n",
        "    max_steps = len(idxes)\n",
        "    n_steps = 200\n",
        "    progbar = False\n",
        "\n",
        "    do_steps = int(max_steps * 1.0)\n",
        "    parts = np.linspace(0, 1, n_steps)\n",
        "    parts_int = [int(p) for p in np.round(parts*max_steps)]\n",
        "    min_value = initial_val\n",
        "    min_degraded_t = img_t\n",
        "\n",
        "    parts = [0]\n",
        "    perturbed_ts = [img_t]\n",
        "    for step in tqdm(range(do_steps), desc=\"Perturbing\", disable=not progbar):\n",
        "      perturber.perturbe(*idxes[step])\n",
        "      if step in parts_int:\n",
        "          perturbed_ts.append(perturber.get_current().clone())\n",
        "\n",
        "    perturbed_ts = torch.cat(perturbed_ts, 0)\n",
        "    with torch.no_grad():\n",
        "      model_results = to_np(vgg(perturbed_ts))\n",
        "\n",
        "    model_results = model_results[:, top1]\n",
        "    model_results = np.array(model_results)\n",
        "    model_results = model_results/max(model_results)\n",
        "    save_model_results.append(model_results)\n",
        "\n",
        "  \n",
        "np_result = np.array(save_model_results)\n",
        "result = np_result.mean(axis=0)\n",
        "result_normalize = (result-min(result))/(max(result) - min(result)) \n",
        "\n",
        "x = np.array(range(len(result)))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"IBA\" + \",AUC=\" + str(round(auc(x, result_normalize)/(result_normalize[0]*len(result_normalize)),3)))\n",
        "plt.ylabel('normalized model score')\n",
        "plt.xlabel('level of degradation[%]')\n",
        "plt.ylim([-0.05,1.05])\n",
        "plt.yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "plt.xticks([0, 20, 40, 60, 80, 100])\n",
        "new_x = x/max(x)*100\n",
        "plt.plot(new_x, result_normalize)\n",
        "plt.savefig(\"\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J71V3CMu6xtj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}