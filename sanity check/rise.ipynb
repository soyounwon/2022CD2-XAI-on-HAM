{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bT8fb2-2LNa_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage.transform import resize\n",
        "from tqdm import tqdm\n",
        "from keras import backend as K\n",
        "from torchvision import models\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "from visualize import *\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dab7dW_bPhMi"
      },
      "outputs": [],
      "source": [
        "def generate_masks(N, s, p1):\n",
        "    cell_size = np.ceil(np.array(model.input_size) / s)\n",
        "    up_size = (s + 1) * cell_size\n",
        "\n",
        "    grid = np.random.rand(N, s, s) < p1\n",
        "    grid = grid.astype('float32')\n",
        "\n",
        "    masks = np.empty((N, *model.input_size))\n",
        "\n",
        "    for i in tqdm(range(N), desc='Generating masks'):\n",
        "        # Random shifts\n",
        "        x = np.random.randint(0, cell_size[0])\n",
        "        y = np.random.randint(0, cell_size[1])\n",
        "        # Linear upsampling and cropping\n",
        "        masks[i, :, :] = resize(grid[i], up_size, order=1, mode='reflect',\n",
        "                                anti_aliasing=False)[x:x + model.input_size[0], y:y + model.input_size[1]]\n",
        "    masks = masks.reshape(-1, *model.input_size, 1)\n",
        "    return masks\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "def explain(model, inp, masks):\n",
        "    preds = []\n",
        "    # Make sure multiplication is being done for correct axes\n",
        "    masked = inp * masks\n",
        "    for i in tqdm(range(0, N, batch_size), desc='Explaining'):\n",
        "        preds.append(model.run_on_batch(masked[i:min(i+batch_size, N)]))\n",
        "    preds = np.concatenate(preds)\n",
        "    sal = preds.T.dot(masks.reshape(N, -1)).reshape(-1, *model.input_size)\n",
        "    sal = sal / N / p1\n",
        "    return sal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pMxFOHlIu4P"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
        "# from keras import backend as K\n",
        "from keras.preprocessing import image\n",
        "\n",
        "class Model():\n",
        "    def __init__(self):\n",
        "      K.set_learning_phase(0)\n",
        "      self.model = models.vgg16(pretrained=True)\n",
        "      # self.model = VGG16(weights='imagenet')\n",
        "      self.input_size = (224, 224)\n",
        "\n",
        "    def run_on_batch(self, x):\n",
        "      print('run on batch')\n",
        "      # x = torch.Tensor(x)      \n",
        "      # return self.model(x)\n",
        "      return self.model.predict(x)\n",
        "\n",
        "def load_img(path):\n",
        "  # img = image.load_img(path)\n",
        "  img = cv2.imread(path)\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  return img, x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kkvnf_k7RILp"
      },
      "outputs": [],
      "source": [
        "imagePath = []\n",
        "targets = [58,222,332,417,429,429,525,549,683,889,990]\n",
        "features_ls = [30,28,26,24,21,19,17,14,12,10,7,5,2,0]\n",
        "\n",
        "trained = Model()\n",
        "untrained = models.vgg16(pretrained=False)\n",
        "\n",
        "for i, path in enumerate(imagePath):\n",
        "  N = 2000\n",
        "  s = 8\n",
        "  p1 = 0.5\n",
        "  masks = generate_masks(N, s, p1)\n",
        "\n",
        "  img, x = (path)\n",
        "  \n",
        "  for layer in features_ls:\n",
        "    if layer == 28:\n",
        "      trained.classifier = untrained.classifier\n",
        "    if layer < 30:\n",
        "      trained.features[layer] = untrained.features[layer]\n",
        "\n",
        "    sal = explain(trained, x, masks)\n",
        "\n",
        "    save_path = ''\n",
        "    save_result(sal, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "compute SSIM score"
      ],
      "metadata": {
        "id": "vZ013WtKj3-D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_X_CjaDzr75"
      },
      "outputs": [],
      "source": [
        "base_path = ''\n",
        "\n",
        "ssim_ls = compute_ssim(args.image_path, base_path) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHZlaBwdy5Ee"
      },
      "outputs": [],
      "source": [
        "plot_ssim(ssim_ls, 'RISE', '')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RISE_imagenet_sample_sanityCheck.ipynb의 사본",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}